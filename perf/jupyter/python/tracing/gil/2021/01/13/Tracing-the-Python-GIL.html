<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Tracing the Python GIL | Maarten Breddels’ blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Tracing the Python GIL" />
<meta name="author" content="Maarten Breddels" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Tracing and visualizing the Python GIL with perf and VizTracer" />
<meta property="og:description" content="Tracing and visualizing the Python GIL with perf and VizTracer" />
<link rel="canonical" href="https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/13/Tracing-the-Python-GIL.html" />
<meta property="og:url" content="https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/13/Tracing-the-Python-GIL.html" />
<meta property="og:site_name" content="Maarten Breddels’ blog" />
<meta property="og:image" content="https://www.maartenbreddels.com/images/example1-uprobes.png" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2021-01-13T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/13/Tracing-the-Python-GIL.html","@type":"BlogPosting","headline":"Tracing the Python GIL","dateModified":"2021-01-13T00:00:00-06:00","datePublished":"2021-01-13T00:00:00-06:00","image":"https://www.maartenbreddels.com/images/example1-uprobes.png","mainEntityOfPage":{"@type":"WebPage","@id":"https://www.maartenbreddels.com/perf/jupyter/python/tracing/gil/2021/01/13/Tracing-the-Python-GIL.html"},"author":{"@type":"Person","name":"Maarten Breddels"},"description":"Tracing and visualizing the Python GIL with perf and VizTracer","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://www.maartenbreddels.com/feed.xml" title="Maarten Breddels' blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Maarten Breddels&#39; blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/projects/">Projects</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Tracing the Python GIL</h1><p class="page-description">Tracing and visualizing the Python GIL with perf and VizTracer</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-01-13T00:00:00-06:00" itemprop="datePublished">
        Jan 13, 2021
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Maarten Breddels</span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      20 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#perf">perf</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#jupyter">jupyter</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#tracing">tracing</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#GIL">GIL</a>
        
      
      </p>
    

    
      
        <div class="pb-5 d-flex flex-wrap flex-justify-end">
          <div class="px-2">

    <a href="https://github.com/maartenbreddels/fastblog/tree/master/_notebooks/2021-14-01-Tracing-the-Python-GIL.ipynb" role="button" target="_blank">
<img class="notebook-badge-image" src="/assets/badges/github.svg" alt="View On GitHub">
    </a>
</div>

          <div class="px-2">
    <a href="https://mybinder.org/v2/gh/maartenbreddels/fastblog/master?filepath=_notebooks%2F2021-14-01-Tracing-the-Python-GIL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/binder.svg" alt="Open In Binder"/>
    </a>
</div>

          <div class="px-2">
    <a href="https://colab.research.google.com/github/maartenbreddels/fastblog/blob/master/_notebooks/2021-14-01-Tracing-the-Python-GIL.ipynb" target="_blank">
        <img class="notebook-badge-image" src="/assets/badges/colab.svg" alt="Open In Colab"/>
    </a>
</div>
        </div>
      </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#Why-the-GIL-matters">Why the GIL matters </a></li>
<li class="toc-entry toc-h1"><a href="#Why-I-write-this">Why I write this </a></li>
<li class="toc-entry toc-h1"><a href="#Prerequisites">Prerequisites </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Linux">Linux </a></li>
<li class="toc-entry toc-h2"><a href="#Perf">Perf </a></li>
<li class="toc-entry toc-h2"><a href="#Kernel-configuration">Kernel configuration </a></li>
<li class="toc-entry toc-h2"><a href="#Python-packages">Python packages </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Track-thread-or-process-states-via-perf">Track thread or process states via perf </a></li>
<li class="toc-entry toc-h1"><a href="#VizTracer">VizTracer </a></li>
<li class="toc-entry toc-h1"><a href="#Mixing-VizTracer-and-perf-output">Mixing VizTracer and perf output </a></li>
<li class="toc-entry toc-h1"><a href="#Detecting-the-GIL">Detecting the GIL </a>
<ul>
<li class="toc-entry toc-h2"><a href="#Via-stack-traces">Via stack traces </a></li>
<li class="toc-entry toc-h2"><a href="#Via-probes-(kprobes/uprobes)">Via probes (kprobes/uprobes) </a></li>
</ul>
</li>
<li class="toc-entry toc-h1"><a href="#Releasing-the-Kraken...-ehm-GIL">Releasing the Kraken... ehm GIL </a></li>
<li class="toc-entry toc-h1"><a href="#Jupyter-integration">Jupyter integration </a></li>
<li class="toc-entry toc-h1"><a href="#Conclusion">Conclusion </a></li>
<li class="toc-entry toc-h1"><a href="#TLDR">TLDR </a></li>
<li class="toc-entry toc-h1"><a href="#Future-plans">Future plans </a></li>
</ul><!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-14-01-Tracing-the-Python-GIL.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Why-the-GIL-matters">
<a class="anchor" href="#Why-the-GIL-matters" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why the GIL matters<a class="anchor-link" href="#Why-the-GIL-matters"> </a>
</h1>
<p>There are plenty of articles explaining why the Python GIL (The Global Interpreter Lock) exists<sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup>, and why it is there. The TLDR version is: the GIL prevents multithreaded pure Python code from using multiple CPU cores.</p>
<p>However, in <a href="https://vaex.io">Vaex</a> we execute most of the CPU intensive parts in C (C++) code, where we release the GIL. This is a common practice in high-performance Python libraries, where Python acts merely as a high-level glue.</p>
<p>However, the GIL needs to be released explicitly, and this is the responsibility of the programmer and might be forgotten, leading to suboptimal use of your machine.</p>
<p>I recently had this issue in <a href="https://github.com/vaexio/vaex/pull/1114">Vaex</a> where I simply forgot to release the GIL and found a similar issue in <a href="https://github.com/apache/arrow/pull/7756">Apache Arrow</a><sup class="footnote-ref" id="fnref-2"><a href="#fn-2">2</a></sup>.</p>
<p>Also, when running on 64 cores, I sometimes see a performance in Vaex that I am not happy with. It might be using 4000% CPU, instead of 6400% CPU, which is something I am not happy with. Instead of blindly pulling some levers to inspect the effect, I want to understand what is happening, and if the GIL is the problem, why, and where is it holding Vaex down.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-1"><p>I am assuming CPython.<a href="#fnref-1" class="footnote">↩</a></p></li>
<li id="fn-2"><p>Apache Arrow is a dependency of Vaex, so anytime the GIL is not released in Arrow, we (and others) suffer a performance hit.<a href="#fnref-2" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Why-I-write-this">
<a class="anchor" href="#Why-I-write-this" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why I write this<a class="anchor-link" href="#Why-I-write-this"> </a>
</h1>
<p>I'm planning to write a series of articles explaining some tools and techniques available for profiling/tracing Python together with native extensions, and how these tools can be glued together, to analyze and visualize what Python is doing, and when the GIL it taken or dropped.</p>
<p>I hope this leads to improvement of tracing, profiling, and other performance tooling in the Python ecosystem, and the performance of the whole Python ecosystem.</p>
<h1 id="Prerequisites">
<a class="anchor" href="#Prerequisites" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prerequisites<a class="anchor-link" href="#Prerequisites"> </a>
</h1>
<h2 id="Linux">
<a class="anchor" href="#Linux" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linux<a class="anchor-link" href="#Linux"> </a>
</h2>
<p>Get access to a Linux machine, and make sure you have root privileges (sudo is fine), or ask your sysadmin to execute some of these commands for you. For the rest of the document, we only run as user.</p>
<h2 id="Perf">
<a class="anchor" href="#Perf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Perf<a class="anchor-link" href="#Perf"> </a>
</h2>
<p>Make sure you have perf installed, e.g. on Ubuntu:</p>

<pre><code>$ sudo yum install perf</code></pre>
<h2 id="Kernel-configuration">
<a class="anchor" href="#Kernel-configuration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Kernel configuration<a class="anchor-link" href="#Kernel-configuration"> </a>
</h2>
<p>To enable running it as a user:</p>

<pre><code># Enable users to run perf (use at own risk)
$ sudo sysctl kernel.perf_event_paranoid=-1

# Enable users to see schedule trace events:
$ sudo mount -o remount,mode=755 /sys/kernel/debug
$ sudo mount -o remount,mode=755 /sys/kernel/debug/tracing</code></pre>
<h2 id="Python-packages">
<a class="anchor" href="#Python-packages" aria-hidden="true"><span class="octicon octicon-link"></span></a>Python packages<a class="anchor-link" href="#Python-packages"> </a>
</h2>
<p>We will make use of <a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a> and <a href="https://github.com/maartenbreddels/per4m">per4m</a></p>

<pre><code>$ pip install "viztracer&gt;=0.11.2" "per4m&gt;=0.1,&lt;0.2"</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Track-thread-or-process-states-via-perf">
<a class="anchor" href="#Track-thread-or-process-states-via-perf" aria-hidden="true"><span class="octicon octicon-link"></span></a>Track thread or process states via perf<a class="anchor-link" href="#Track-thread-or-process-states-via-perf"> </a>
</h1>
<p>There is no way to get the GIL state in Python <sup class="footnote-ref" id="fnref-3"><a href="#fn-3">1</a></sup> since there is no API for this. We can track it from the kernel, and the right tool for this under Linux is <strong>perf</strong>.</p>
<p>Using the linux perf tool (aka perf_events), we can listen to the state changes for processes/threads (we only care about sleeping and running), and log them. Although perf may look scary, it is a powerful tool. If you want to know a bit more about perf, I recommend reading <a href="https://jvns.ca/blog/2018/04/16/new-perf-zine/">Julia Evans' zine on perf</a> or <a href="http://www.brendangregg.com/perf.html">go through Brendan Gregg's website</a>.</p>
<p>To build our intuition, we will first run perf on a <a href="https://github.com/maartenbreddels/per4m/blob/master/per4m/example0.py">very trivial program</a>:</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-3"><p>Except using polling <a href="https://github.com/chrisjbillington/gil_load/">https://github.com/chrisjbillington/gil_load/</a><a href="#fnref-3" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">time</span>
<span class="kn">from</span> <span class="nn">threading</span> <span class="kn">import</span> <span class="n">Thread</span>


<span class="k">def</span> <span class="nf">sleep_a_bit</span><span class="p">():</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">sleep_a_bit</span><span class="p">)</span>
    <span class="n">t</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">t</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>


<span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We listen to just a few events to keep the noise down (note the use of wildcards):</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork \
        -e 'sched:sched_wak*' -- python -m per4m.example0
[ perf record: Woken up 2 times to write data ]
[ perf record: Captured and wrote 0,032 MB perf.data (33 samples) ]</code></pre>
<p>And use the <code>perf script</code> command to write human/parsable output.</p>

<pre><code>$ perf script
        :3040108 3040108 [032] 5563910.979408:                sched:sched_waking: comm=perf pid=3040114 prio=120 target_cpu=031
        :3040108 3040108 [032] 5563910.979431:                sched:sched_wakeup: comm=perf pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995616:                sched:sched_waking: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995618:                sched:sched_wakeup: comm=kworker/31:1 pid=2502104 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995621:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995622:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563910.995624:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R+ ==&gt; next_comm=kworker/31:1 next_pid=2502104 next_prio=120
          python 3040114 [031] 5563911.003612:                sched:sched_waking: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032
          python 3040114 [031] 5563911.003614:                sched:sched_wakeup: comm=kworker/32:1 pid=2467833 prio=120 target_cpu=032
          python 3040114 [031] 5563911.083609:                sched:sched_waking: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563911.083612:                sched:sched_wakeup: comm=ksoftirqd/31 pid=198 prio=120 target_cpu=031
          python 3040114 [031] 5563911.083613:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=R ==&gt; next_comm=ksoftirqd/31 next_pid=198 next_prio=120
          python 3040114 [031] 5563911.108984:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045
          python 3040114 [031] 5563911.109059:                sched:sched_waking: comm=node pid=2446812 prio=120 target_cpu=045
          python 3040114 [031] 5563911.112250:          sched:sched_process_fork: comm=python pid=3040114 child_comm=python child_pid=3040116
          python 3040114 [031] 5563911.112260:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040114 [031] 5563911.112262:            sched:sched_wakeup_new: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040114 [031] 5563911.112273:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
          python 3040116 [037] 5563911.112418:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112450:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112473: sched:sched_wake_idle_without_ipi: cpu=31
         swapper     0 [031] 5563911.112476:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563911.112485:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
          python 3040116 [037] 5563911.112485:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112489:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563911.112496:                sched:sched_switch: prev_comm=python prev_pid=3040116 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/37 next_pid=0 next_prio=120
         swapper     0 [031] 5563911.112497:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040114 [031] 5563911.112513:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120
         swapper     0 [037] 5563912.113490:                sched:sched_waking: comm=python pid=3040116 prio=120 target_cpu=037
         swapper     0 [037] 5563912.113529:                sched:sched_wakeup: comm=python pid=3040116 prio=120 target_cpu=037
          python 3040116 [037] 5563912.113595:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
          python 3040116 [037] 5563912.113620:                sched:sched_waking: comm=python pid=3040114 prio=120 target_cpu=031
         swapper     0 [031] 5563912.113697:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Take a moment to digest the output. I can see a few things. Looking at the 4th column (time in seconds), we can see where the program slept (it skips 1 second). Here we see that we enter the sleeping state with a line like:</p>
<p><code>python 3040114 [031] 5563911.112513:                sched:sched_switch: prev_comm=python prev_pid=3040114 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/31 next_pid=0 next_prio=120</code></p>
<p>This means the kernel changed the state of the Python thread to <code>S</code> (=sleeping) state.</p>
<p>A full second later, we see it being woken up:</p>
<p><code>swapper     0 [031] 5563912.113697:                sched:sched_wakeup: comm=python pid=3040114 prio=120 target_cpu=031</code></p>
<p>Of course, you need to build some tooling around this, to really see what is happening. But one can imagine this output can be easily parsed and this is what <a href="https://github.com/maartenbreddels/per4m/">per4m</a> does. However, before we go there, I'd first like to visualize the flow of a slightly more advanced program using <a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="VizTracer">
<a class="anchor" href="#VizTracer" aria-hidden="true"><span class="octicon octicon-link"></span></a>VizTracer<a class="anchor-link" href="#VizTracer"> </a>
</h1>
<p><a href="https://github.com/gaogaotiantian/viztracer/">VizTracer</a> is a Python tracer that can visualize what your program does in the browser. Let us run it on a slightly <a href="https://github.com/maartenbreddels/per4m/blob/master/per4m/example1.py">more advanced example</a> to see what it looks like.</p>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>


<span class="k">def</span> <span class="nf">some_computation</span><span class="p">():</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>   
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1_000_000</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">i</span>
    <span class="k">return</span> <span class="n">total</span>


<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
    <span class="n">thread1</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">some_computation</span><span class="p">)</span>
    <span class="n">thread2</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">some_computation</span><span class="p">)</span>
    <span class="n">thread1</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">thread2</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">time</span><span class="o">.</span><span class="n">sleep</span><span class="p">(</span><span class="mf">0.2</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="p">[</span><span class="n">thread1</span><span class="p">,</span> <span class="n">thread2</span><span class="p">]:</span>
        <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>


<span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Running viztracer gives output like:</p>

<pre><code>$ viztracer -o example1.html --ignore_frozen -m per4m.example1
Loading finish                                        
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...
Dumping trace data to json, total entries: 94, estimated json file size: 11.0KiB
Generating HTML report
Report saved.</code></pre>
<p>And the HTML should render as:
<img src="/images/copied_from_nb/per4m/example1.png" alt="image.png"></p>
<p>From this, it seems that <code>some_computation</code> seem to be executed in parallel (twice), while in fact, we know the GIL is preventing that. So what is really going on?</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Mixing-VizTracer-and-perf-output">
<a class="anchor" href="#Mixing-VizTracer-and-perf-output" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mixing VizTracer and perf output<a class="anchor-link" href="#Mixing-VizTracer-and-perf-output"> </a>
</h1>
<p>Let us run <code>perf</code> on this, similarly to what we did to example0.py. However, we add the argument <code>-k CLOCK_MONOTONIC</code> so that we use <a href="https://github.com/gaogaotiantian/viztracer/blob/3321ba4024afe5623f938a601d7f7db3b08f534d/src/viztracer/modules/snaptrace.c#L91">the same clock as VizTracer</a> and ask VizTracer to generate a JSON, instead of an HTML file:</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*' \
   -k CLOCK_MONOTONIC  -- viztracer -o viztracer1.json --ignore_frozen -m per4m.example1</code></pre>
<p>Then we use <code>per4m</code> to translate the perf script results into a JSON that VizTracer can read</p>

<pre><code>$ perf script | per4m perf2trace sched -o perf1.json
Wrote to perf1.json</code></pre>
<p>Next, use VizTracer to combine the two JSON files.</p>

<pre><code>$ viztracer --combine perf1.json viztracer1.json -o example1_state.html
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...
Dumping trace data to json, total entries: 131, estimated json file size: 15.4KiB
Generating HTML report
Report saved.</code></pre>
<p>This gives us:
<img src="/images/copied_from_nb/per4m/example1-state.png" alt="image.png"></p>
<p>From this visualization, it is clear the threads regularly enter a sleeping state due to the GIL and do not execute in parallel.
</p>
<div class="flash">
    <svg class="octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>The length of the sleep phase is ~5ms, which corresponds to the default value of <a href="https://docs.python.org/3/library/sys.html#sys.getswitchinterval">sys.getswitchinterval</a>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Detecting-the-GIL">
<a class="anchor" href="#Detecting-the-GIL" aria-hidden="true"><span class="octicon octicon-link"></span></a>Detecting the GIL<a class="anchor-link" href="#Detecting-the-GIL"> </a>
</h1>
<p>We see our process sleeping, but we do not see any difference between the sleeping state being triggered by calling <code>time.sleep</code> and due to the GIL. There are multiple way to see the difference, and we will present two methods.</p>
<h2 id="Via-stack-traces">
<a class="anchor" href="#Via-stack-traces" aria-hidden="true"><span class="octicon octicon-link"></span></a>Via stack traces<a class="anchor-link" href="#Via-stack-traces"> </a>
</h2>
<p>Using <code>perf record -g</code> (or better <code>perf record --call-graph dwarf</code> which implies <code>-g</code>), we get stack traces for each event of perf.</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*'\
   -k CLOCK_MONOTONIC  --call-graph dwarf -- viztracer -o viztracer1-gil.json --ignore_frozen -m per4m.example1
Loading finish                                        
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/viztracer1-gil.json ...
Dumping trace data to json, total entries: 94, estimated json file size: 11.0KiB
Report saved.
[ perf record: Woken up 3 times to write data ]
[ perf record: Captured and wrote 0,991 MB perf.data (164 samples) ]</code></pre>
<p>Looking at the output of <code>perf script</code> (where we add <code>--no-inline</code> for performance reasons), we get a load of information. Looking at a state change event, we can now see that <a href="https://github.com/python/cpython/blob/1a9f51ed12feb4d95ad6d0faf610a030c05b9f5e/Python/ceval_gil.h#L215">take_gil</a> was called!</p>

<pre><code>$ perf script --no-inline | less
...
viztracer 3306851 [059] 5614683.022539:                sched:sched_switch: prev_comm=viztracer prev_pid=3306851 prev_prio=120 prev_state=S ==&gt; next_comm=swapper/59 next_pid=0 next_prio=120
        ffffffff96ed4785 __sched_text_start+0x375 ([kernel.kallsyms])
        ffffffff96ed4785 __sched_text_start+0x375 ([kernel.kallsyms])
        ffffffff96ed4b92 schedule+0x42 ([kernel.kallsyms])
        ffffffff9654a51b futex_wait_queue_me+0xbb ([kernel.kallsyms])
        ffffffff9654ac85 futex_wait+0x105 ([kernel.kallsyms])
        ffffffff9654daff do_futex+0x10f ([kernel.kallsyms])
        ffffffff9654dfef __x64_sys_futex+0x13f ([kernel.kallsyms])
        ffffffff964044c7 do_syscall_64+0x57 ([kernel.kallsyms])
        ffffffff9700008c entry_SYSCALL_64_after_hwframe+0x44 ([kernel.kallsyms])
            7f4884b977b1 pthread_cond_timedwait@@GLIBC_2.3.2+0x271 (/usr/lib/x86_64-linux-gnu/libpthread-2.31.so)
            55595c07fe6d take_gil+0x1ad (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfaa0b3 PyEval_RestoreThread+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c000872 lock_PyThread_acquire_lock+0x1d2 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfe71f3 _PyMethodDef_RawFastCallKeywords+0x263 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfe7313 _PyCFunction_FastCallKeywords+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d657 call_function+0x3b7 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6b00 _PyFunction_FastCallKeywords+0x520 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6b00 _PyFunction_FastCallKeywords+0x520 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060ae4 _PyEval_EvalFrameDefault+0x3f4 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c074e5d builtin_exec+0x33d (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfe7078 _PyMethodDef_RawFastCallKeywords+0xe8 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfe7313 _PyCFunction_FastCallKeywords+0x23 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c066c39 _PyEval_EvalFrameDefault+0x6549 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb77e0 _PyEval_EvalCodeWithName+0xc80 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6b62 _PyFunction_FastCallKeywords+0x582 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c01d334 call_function+0x94 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060d00 _PyEval_EvalFrameDefault+0x610 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfd6766 _PyFunction_FastCallKeywords+0x186 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c060ae4 _PyEval_EvalFrameDefault+0x3f4 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb6db1 _PyEval_EvalCodeWithName+0x251 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595bfb81e2 PyEval_EvalCode+0x22 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c0c51d1 run_mod+0x31 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c0cf31d PyRun_FileExFlags+0x9d (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c0cf50a PyRun_SimpleFileExFlags+0x1ba (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c0d05f0 pymain_main+0x3e0 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            55595c0d067b _Py_UnixMain+0x3b (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
            7f48849bc0b2 __libc_start_main+0xf2 (/usr/lib/x86_64-linux-gnu/libc-2.31.so)
            55595c075100 _start+0x28 (/home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
...</code></pre>
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>We also see that <code>pthread_cond_timedwait</code> is called, this is what <a href="https://github.com/sumerc/gilstats.py">https://github.com/sumerc/gilstats.py</a> uses for a eBPF tool, in case you are interested in other mutexes.
</div>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>Also note that we do not see Python stacktrace, but <code>_PyEval_EvalFrameDefault</code> etcetera instead. I plan to write how to inject Python stacktraces in a future article.
</div>
<p>The <code>per4m perf2trace</code> convert tool understands this and will generate different output when <code>take_gil</code> is in the stacktrace:</p>

<pre><code>$ perf script --no-inline | per4m perf2trace sched -o perf1-gil.json
Wrote to perf1-gil.json
$ viztracer --combine perf1-gil.json viztracer1-gil.json -o example1-gil.html
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1.html ...
Dumping trace data to json, total entries: 131, estimated json file size: 15.4KiB
Generating HTML report
Report saved.</code></pre>
<p>This gives us:
<img src="/images/copied_from_nb/per4m/example1-gil.png" alt="image.png"></p>
<p>Now we really see where the GIL plays a role!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Via-probes-(kprobes/uprobes)">
<a class="anchor" href="#Via-probes-(kprobes/uprobes)" aria-hidden="true"><span class="octicon octicon-link"></span></a>Via probes (kprobes/uprobes)<a class="anchor-link" href="#Via-probes-(kprobes/uprobes)"> </a>
</h2>
<p>We now know when processes are sleeping (due to the GIL or other reasons), but if we want to get a more detailed look at where the GIL is taken or dropped, we need to know where <code>take_gil</code> and <code>drop_gil</code> are being called as well as returned. This can be traced using uprobes via perf. Uprobes are probes in userland, the equivalent to kprobes, which as you may have guessed operate in kernel space. <a href="https://jvns.ca/blog/2017/07/09/linux-tracing-zine/">Julia Evans</a> is again a great resource.</p>
<p>Let us install the 4 uprobes:</p>

<pre><code>sudo perf probe -f -x `which python` python:take_gil=take_gil
sudo perf probe -f -x `which python` python:take_gil=take_gil%return
sudo perf probe -f -x `which python` python:drop_gil=drop_gil
sudo perf probe -f -x `which python` python:drop_gil=drop_gil%return

Added new events:
  python:take_gil      (on take_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
  python:take_gil_1    (on take_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)

You can now use it in all perf tools, such as:

        perf record -e python:take_gil_1 -aR sleep 1

Failed to find "take_gil%return",
 because take_gil is an inlined function and has no return point.
Added new event:
  python:take_gil__return (on take_gil%return in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)

You can now use it in all perf tools, such as:

        perf record -e python:take_gil__return -aR sleep 1

Added new events:
  python:drop_gil      (on drop_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)
  python:drop_gil_1    (on drop_gil in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)

You can now use it in all perf tools, such as:

        perf record -e python:drop_gil_1 -aR sleep 1

Failed to find "drop_gil%return",
 because drop_gil is an inlined function and has no return point.
Added new event:
  python:drop_gil__return (on drop_gil%return in /home/maartenbreddels/miniconda/envs/dev/bin/python3.7)

You can now use it in all perf tools, such as:

        perf record -e python:drop_gil__return -aR sleep 1</code></pre>
<p>It complains a bit and seems to add multiple probes/events because <code>drop_gil</code> and <code>take_gil</code> are inlined (which means the function is present multiple times in the binary), but it seems to work 🤷 (let me know in the comments if it does not work for you).
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>I may have been lucky, that the Python binary that I use (from conda-forge) is compiled in such a way that the relevant take_gil/drop_gil (and its return) that succeed are the relevant ones for this problem.
</div>
<p>Note that the probes cause no performance hit, only when they are 'active' (like when we monitor them under perf) will the trigger a different code path. When monitored, the affected pages for the monitored process will be copied, and <a href="https://lwn.net/Articles/499190/">breakpoints are inserted at the right locations</a> (INT3 for x86 CPUs). The breakpoint will trigger the event for perf, which causes a small overhead. In case you want to remove the probes, run:</p>

<pre><code>$ sudo perf probe --del 'python*'</code></pre>
<p>Now <code>perf</code> understand new events that it can listen to, so let us run perf again with <code>-e 'python:*gil*'</code> as extra argument</p>

<pre><code>$ perf record -e sched:sched_switch  -e sched:sched_process_fork -e 'sched:sched_wak*' -k CLOCK_MONOTONIC  \
  -e 'python:*gil*' -- viztracer  -o viztracer1-uprobes.json --ignore_frozen -m per4m.example1</code></pre>
<p></p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong>we removed <code>--call-graph dwarf</code> otherwise perf cannot keep up, and we will lose events.
</div>
<p>We then use <code>per4m perf2trace</code> to convert this to a JSON that VizTracer understands, while we also get some free statistics.</p>

<pre><code>$ perf script --no-inline | per4m perf2trace gil -o perf1-uprobes.json
...
Summary of threads:

PID         total(us)    no gil%✅    has gil%❗    gil wait%❌
--------  -----------  -----------  ------------  -------------
3353567*     164490.0         65.9          27.3            6.7
3353569       66560.0          0.3          48.2           51.5
3353570       60900.0          0.0          56.4           43.6

High 'no gil' is good (✅), we like low 'has gil' (❗),
 and we don't want 'gil wait' (❌). (* indicates main thread)
... 
Wrote to perf1-uprobes.json</code></pre>
<p>Note that the <code>per4m perf2trace gil</code> subcommand also gives a <a href="https://github.com/chrisjbillington/gil_load">gil_load</a> like output. From this output, we see that both threads are waiting for the GIL approximately 50% of the time, as expected.</p>
<p>Using the same <code>perf.data</code> file, that <code>perf record</code> generated, we can also generate the thread/process state information. However, because we ran without the stacktraces, we do not know if we are sleeping due to the GIL or not.</p>

<pre><code>$ perf script --no-inline | per4m perf2trace sched -o perf1-state.json
Wrote to perf1-state.json</code></pre>
<p>At last, we combine the three outputs:</p>

<pre><code>$ viztracer --combine perf1-state.json perf1-uprobes.json viztracer1-uprobes.json -o example1-uprobes.html 
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example1-uprobes.html ...
Dumping trace data to json, total entries: 10484, estimated json file size: 1.2MiB
Generating HTML report
Report saved.</code></pre>
<p>Our VizTracer output gives us a good overview of who has, and wants the GIL:
<img src="/images/copied_from_nb/per4m/example1-uprobes.png" alt="image.png"></p>
<p>Above each thread, we see when a thread/process wants to take the gil, and when it succeeded (marked by <code>LOCK</code>). Note that these periods overlap with the periods when the thread/process is <em>not</em> sleeping (so running!). Also note that we see only 1 thread/process in the running state, as expected due to the GIL.</p>
<p>The time between each call to <code>take_gil</code>, and actually obtaining the lock (and thus leaving the sleeping state, or waking up), is exactly the time in the above table in the column <code>gil wait%❌</code>. The time each thread has the GIL, or the time spanned by <code>LOCK</code>, is the time in the column <code>has gil%❗</code>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Releasing-the-Kraken...-ehm-GIL">
<a class="anchor" href="#Releasing-the-Kraken...-ehm-GIL" aria-hidden="true"><span class="octicon octicon-link"></span></a>Releasing the Kraken... ehm GIL<a class="anchor-link" href="#Releasing-the-Kraken...-ehm-GIL"> </a>
</h1>
<p>We saw a pure Python program, running multithreaded, where the GIL is limiting performance by letting only 1 thread/process run at a time <sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup>. Let us now see what happens when the code releases the GIL, such as what happens when we execute NumPy functions.</p>
<p>The second example execute <code>some_numpy_computation</code>, which calls a NumPy function <code>M=4</code> times, in parallel using 2 threads, while the main thread executes pure Python code.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-1"><p>Per Python process of course, and possibly in the future per (sub)interpreter.<a href="#fnref-1" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">threading</span>
<span class="kn">import</span> <span class="nn">time</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="n">N</span> <span class="o">=</span> <span class="mi">1024</span><span class="o">*</span><span class="mi">1024</span><span class="o">*</span><span class="mi">32</span>
<span class="n">M</span> <span class="o">=</span> <span class="mi">4</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">'f8'</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">some_numpy_computation</span><span class="p">():</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">M</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">total</span>



<span class="k">def</span> <span class="nf">main</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">thread1</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">some_numpy_computation</span><span class="p">)</span>
    <span class="n">thread2</span> <span class="o">=</span> <span class="n">threading</span><span class="o">.</span><span class="n">Thread</span><span class="p">(</span><span class="n">target</span><span class="o">=</span><span class="n">some_numpy_computation</span><span class="p">)</span>
    <span class="n">thread1</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">thread2</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
    <span class="n">total</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2_000_000</span><span class="p">):</span>
        <span class="n">total</span> <span class="o">+=</span> <span class="n">i</span>
    <span class="k">for</span> <span class="n">thread</span> <span class="ow">in</span> <span class="p">[</span><span class="n">thread1</span><span class="p">,</span> <span class="n">thread2</span><span class="p">]:</span>
        <span class="n">thread</span><span class="o">.</span><span class="n">join</span><span class="p">()</span>
<span class="n">main</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Instead of running this script using perf and VizTracer, we now use the per4m <code>giltracer</code> util, which automates all the steps done above (in a slightly smarter way<sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup>):</p>

<pre><code>$ giltracer --state-detect -o example2-uprobes.html -m per4m.example2
...

Summary of threads:

PID         total(us)    no gil%✅    has gil%❗    gil wait%❌
--------  -----------  -----------  ------------  -------------
3373601*    1359990.0         95.8           4.2            0.1
3373683       60276.4         84.6           2.2           13.2
3373684       57324.0         89.2           1.9            8.9

High 'no gil' is good (✅), we like low 'has gil' (❗),
 and we don't want 'gil wait' (❌). (* indicates main thread)
...
Saving report to /home/maartenbreddels/github/maartenbreddels/per4m/example2-uprobes.html ...
...</code></pre>
<p><img src="/images/copied_from_nb/per4m/example2-uprobes.png" alt="image.png"></p>
<p>We see that while the main thread is running Python code (it has the GIL, indicated by the <code>LOCK</code> above it), the threads are also running. Note that in the pure Python example we only had one thread/process running at a time. While here we see moments where 3 threads are truly running parallel). This is possible because the NumPy routines that enter into C/C++/Fortran, released the GIL.</p>
<p>However, the threads are still hampered by the GIL, since once the NumPy function returns to Python land, it needs to obtain the GIL again as can be seen by the <code>take_gil</code> blocks taking a bit of time. This causes a 10% wait time for each thread.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-1"><p>We actually start perf twice, once without stacktraces, once with, and we import the module/script before we execute its main function, to get rid of uninteresting tracing information, such as importing modules. This run fast enough that we do not lose events.<a href="#fnref-1" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Jupyter-integration">
<a class="anchor" href="#Jupyter-integration" aria-hidden="true"><span class="octicon octicon-link"></span></a>Jupyter integration<a class="anchor-link" href="#Jupyter-integration"> </a>
</h1>
<p>Since my workflow often involves working from a MacBook laptop<sup class="footnote-ref" id="fnref-1"><a href="#fn-1">1</a></sup> remotely connected to a Linux computer, I use the Jupyter notebook often to execute code remotely. Being a Jupyter developer, creating a cell magic to wrap this was mandatory.</p>
<div class="footnotes">
<hr>
<ol>
<li id="fn-1"><p>Which does not run perf, but supports dtrace, but this is for another time.<a href="#fnref-1" class="footnote">↩</a></p></li>
</ol>
</div>

</div>
</div>
</div>
    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># this registers the giltracer cell magic</span>
<span class="o">%</span><span class="k">load_ext</span> per4m
</pre></div>

    </div>
</div>
</div>

</div>
    

    
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="o">%%</span><span class="k">giltracer</span>
# this call the main define above, but this can also be a multiline code cell
main()
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Saving report to /tmp/tmpvi8zw9ut/viztracer.json ...
Dumping trace data to json, total entries: 117, estimated json file size: 13.7KiB
Report saved.

[ perf record: Woken up 1 times to write data ]
[ perf record: Captured and wrote 0,094 MB /tmp/tmpvi8zw9ut/perf.data (244 samples) ]

Wait for perf to finish...
perf script -i /tmp/tmpvi8zw9ut/perf.data --no-inline --ns | per4m perf2trace gil -o /tmp/tmpvi8zw9ut/giltracer.json -q -v 
Saving report to /home/maartenbreddels/github/maartenbreddels/fastblog/_notebooks/giltracer.html ...
Dumping trace data to json, total entries: 849, estimated json file size: 99.5KiB
Generating HTML report
Report saved.
</pre>
</div>
</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<a href="giltracer.html" download="">Download giltracer.html</a>
</div>

</div>

<div class="output_area">


<div class="output_html rendered_html output_subarea ">
<a href="giltracer.html" target="_blank" rel="noopener noreferrer">Open giltracer.html in new tab</a> (might not work due to security issue)
</div>

</div>

</div>
</div>

</div>
    

<div class="cell border-box-sizing text_cell rendered">
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusion">
<a class="anchor" href="#Conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion<a class="anchor-link" href="#Conclusion"> </a>
</h1>
<p>Using perf, we can detect process/thread states which gives us an idea which thread/process has the GIL in Python. Using stacktraces, we can find out if the sleeping states are really due to the GIL, and not due to <code>time.sleep</code> for instance.</p>
<p>Combining uprobes with perf, we can trace the calling and returning of the <code>take_gil</code> and <code>drop_gil</code> functions, getting an even better view on the effect of the GIL on your Python program.</p>
<p>The <code>per4m</code> Python package, is an experimental package to do some of the <code>perf script</code> to VizTracer JSON format, and some orchestration tools to make this easier to work with.</p>
<h1 id="TLDR">
<a class="anchor" href="#TLDR" aria-hidden="true"><span class="octicon octicon-link"></span></a>TLDR<a class="anchor-link" href="#TLDR"> </a>
</h1>
<p>If you just want to see where the GIL matters:</p>
<p>Run this once:</p>

<pre><code>sudo yum install perf
sudo sysctl kernel.perf_event_paranoid=-1
sudo mount -o remount,mode=755 /sys/kernel/debug
sudo mount -o remount,mode=755 /sys/kernel/debug/tracing
sudo perf probe -f -x `which python` python:take_gil=take_gil
sudo perf probe -f -x `which python` python:take_gil=take_gil%return
sudo perf probe -f -x `which python` python:drop_gil=drop_gil
sudo perf probe -f -x `which python` python:drop_gil=drop_gil%return
pip install "viztracer&gt;=0.11.2" "per4m&gt;=0.1,&lt;0.2"</code></pre>
<p>Example usage:</p>

<pre><code># module
$ giltracer per4m/example2.py
# script
$ giltracer -m per4m.example2
# add thread/process state detection
$ giltracer --state-detect -m per4m.example2
# without uprobes (in case that fails)
$ giltracer --state-detect --no-gil-detect -m per4m.example2</code></pre>
<h1 id="Future-plans">
<a class="anchor" href="#Future-plans" aria-hidden="true"><span class="octicon octicon-link"></span></a>Future plans<a class="anchor-link" href="#Future-plans"> </a>
</h1>
<p>I wish I did not have to develop these tools, and hope I inspire other to build better tooling, that will deprecate mine. I want to write high-performance code, and focus on that.</p>
<p>However, I do see some options that I plan to write about in the future:</p>
<ul>
<li>Expose hardware performance counter in VizTracer, to see for instance cache misses, or CPU stalling.</li>
<li>Inject Python stacktraces into perf stacktraces, so we can combine them with tools from <a href="http://www.brendangregg.com/">http://www.brendangregg.com/</a> .e.g. <a href="http://www.brendangregg.com/offcpuanalysis.html">http://www.brendangregg.com/offcpuanalysis.html</a>
</li>
<li>Repeat the same exercise using dtrace for usage under MacOS.</li>
<li>Automatically detect which C function does <em>not</em> release the GIL (basically autodetect <a href="https://github.com/vaexio/vaex/pull/1114">https://github.com/vaexio/vaex/pull/1114</a> <a href="https://github.com/apache/arrow/pull/7756">https://github.com/apache/arrow/pull/7756</a> )</li>
<li>Apply these to more issues, like in <a href="https://github.com/h5py/h5py/issues/1516">https://github.com/h5py/h5py/issues/1516</a>
</li>
</ul>
<p>If you have other ideas, want to pick some of this up, leave a message or contact me.
</p>
<div class="flash">
    <svg class="octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info" viewbox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z"></path></svg>
    <strong>Note: </strong><a href="https://github.com/maartenbreddels/per4m">https://github.com/maartenbreddels/per4m</a> is under a permissive MIT license, feel free to make use of that!
</div>

</div>
</div>
</div>
</div>



  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="maartenbreddels/fastblog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/perf/jupyter/python/tracing/gil/2021/01/13/Tracing-the-Python-GIL.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Python, vaex</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/maartenbreddels" title="maartenbreddels"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/maartenbreddels" title="maartenbreddels"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
